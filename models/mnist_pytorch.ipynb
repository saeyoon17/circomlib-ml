{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea04cc0-683a-4535-a33e-8b973beacda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import torchvision\n",
    " \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    " \n",
    "train_dataset = torchvision.datasets.MNIST('../dataset/', train=True, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST('../dataset/', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303dcb75-3c7c-4ca2-a04d-a39537e157b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape :  torch.Size([60000, 28, 28])\n",
      "y_train shape :  torch.Size([60000])\n",
      "x_test  shape :  torch.Size([10000, 28, 28])\n",
      "y_test  shape :  torch.Size([10000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "x_train_orig = train_dataset.train_data\n",
    "y_train_orig = train_dataset.train_labels\n",
    "x_test_orig = test_dataset.test_data\n",
    "y_test_orig = test_dataset.test_labels\n",
    "print('x_train shape : ', x_train_orig.shape)\n",
    "print('y_train shape : ', y_train_orig.shape)\n",
    "print('x_test  shape : ', x_test_orig.shape)\n",
    "print('y_test  shape : ', y_test_orig.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65060060-52c9-4419-a7b1-72fd3edd2418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape :  torch.Size([50000, 28, 28])\n",
      "y_train shape :  torch.Size([50000])\n",
      "x_train shape :  torch.Size([10000, 28, 28])\n",
      "y_train shape :  torch.Size([10000])\n",
      "x_test  shape :  torch.Size([10000, 28, 28])\n",
      "y_test  shape :  torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# data normalization\n",
    "x_train = (x_train_orig[10000:] / 255.).to(device=device)\n",
    "y_train = y_train_orig[10000:].to(device=device)\n",
    "x_val = (x_train_orig[:10000] / 255.).to(device=device)\n",
    "y_val = y_train_orig[:10000].to(device=device)\n",
    "x_test = x_test_orig / 255.\n",
    "y_test = y_test_orig.to(device=device)\n",
    " \n",
    "print('x_train shape : ', x_train.size())\n",
    "print('y_train shape : ', y_train.size())\n",
    "print('x_train shape : ', x_val.size())\n",
    "print('y_train shape : ', y_val.size())\n",
    "print('x_test  shape : ', x_test.size())\n",
    "print('y_test  shape : ', y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b7a411-4b15-4577-bee0-9362117dc5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(trainingset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e5069c7-638e-48c2-aff7-7b1d0576029b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model1(\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc5): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc6): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(784, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 256)\n",
    "        self.fc3 = torch.nn.Linear(256, 256)\n",
    "        self.fc4 = torch.nn.Linear(256, 128)\n",
    "        self.fc5 = torch.nn.Linear(128, 128)\n",
    "        self.fc6 = torch.nn.Linear(128, 10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    " \n",
    "m1 = Model1()\n",
    "m1.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca7a397-f917-4a51-a556-bdb3a79713e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.373476028442383 sec - loss : 0.14350922405719757 / acc : 96.15999603271484\n",
      "7.841026782989502 sec - loss : 0.11628090590238571 / acc : 96.93599700927734\n",
      "7.807612895965576 sec - loss : 0.08688635379076004 / acc : 97.6780014038086\n",
      "7.796353101730347 sec - loss : 0.05559922009706497 / acc : 98.50199890136719\n",
      "7.862710952758789 sec - loss : 0.04879147559404373 / acc : 98.65599822998047\n",
      "7.751830816268921 sec - loss : 0.045437026768922806 / acc : 98.75599670410156\n",
      "7.72198486328125 sec - loss : 0.03608589619398117 / acc : 98.9959945678711\n",
      "7.6609251499176025 sec - loss : 0.038856394588947296 / acc : 98.86399841308594\n",
      "9.198440551757812 sec - loss : 0.028890419751405716 / acc : 99.25399780273438\n",
      "8.619903087615967 sec - loss : 0.028300121426582336 / acc : 99.10999298095703\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(m1.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    " \n",
    "    for xb, yb in train_loader:\n",
    "        xb.to('cuda')\n",
    "        yb.to('cuda')\n",
    "        pred = m1(xb)\n",
    "        loss = criterion(pred, yb)\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = m1(x_train)\n",
    "        acc = pred.data.max(1)[1].eq(y_train.data).sum()/len(x_train) * 100\n",
    "        loss = criterion(pred, y_train)\n",
    "    print(f\"{time.time() - start} sec - loss : {loss} / acc : {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a2c2fb4-928d-486e-b9e8-ae6b432c6803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.11915184557437897 / acc : 97.50999450683594\n"
     ]
    }
   ],
   "source": [
    "pred = m1(x_test.to(device=device))\n",
    "acc = pred.data.max(1)[1].eq(y_test.to(device=device).data).sum()/len(x_test) * 100\n",
    "loss = criterion(pred, y_test.to(device=device))\n",
    "print(f\"loss : {loss} / acc : {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "066702b1-8cff-4cb2-acd7-eccdf4adb038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, 3, 1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 16, 2, 1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(784, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = torch.nn.functional.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    " \n",
    "m2 = CNN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0c1902f-57c6-4016-a6c6-1a1b76eadb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 28, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15ca288-f28f-4ab3-b71f-a7bee13ed126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]         200,960\n",
      "              ReLU-2                  [-1, 256]               0\n",
      "            Linear-3                  [-1, 256]          65,792\n",
      "              ReLU-4                  [-1, 256]               0\n",
      "            Linear-5                  [-1, 256]          65,792\n",
      "              ReLU-6                  [-1, 256]               0\n",
      "            Linear-7                  [-1, 128]          32,896\n",
      "              ReLU-8                  [-1, 128]               0\n",
      "            Linear-9                  [-1, 128]          16,512\n",
      "             ReLU-10                  [-1, 128]               0\n",
      "           Linear-11                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 383,242\n",
      "Trainable params: 383,242\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 149.54\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 1.46\n",
      "Estimated Total Size (MB): 151.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torchsummary.summary(m1, input_size=x_train.size())\n",
    "# m2.cuda()\n",
    "# torchsummary.summary(m2, input_size=x_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd626c0-db08-48d2-911c-a4526babfffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('fc1',\n",
       "               Linear(in_features=784, out_features=256, bias=True)),\n",
       "              ('fc2', Linear(in_features=256, out_features=256, bias=True)),\n",
       "              ('fc3', Linear(in_features=256, out_features=256, bias=True)),\n",
       "              ('fc4', Linear(in_features=256, out_features=128, bias=True)),\n",
       "              ('fc5', Linear(in_features=128, out_features=128, bias=True)),\n",
       "              ('fc6', Linear(in_features=128, out_features=10, bias=True)),\n",
       "              ('relu', ReLU())])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c0c75-b271-4f9f-9ebb-37f1bf6906d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ce8af-bdad-40ce-90d8-3ac2b9e6efa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fc3b8-3690-4b96-bdb8-7196b51d9957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e6981-3938-4203-b1fa-ee4ce548bffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966d38a-d21a-4702-a29d-48a74cecba69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb698f49-a808-4406-af4d-15680a57d01a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m xb\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m yb\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, yb)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mCNN_Model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmax_pool2d(x, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(m1.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    " \n",
    "    for xb, yb in train_loader:\n",
    "        xb.to('cuda')\n",
    "        yb.to('cuda')\n",
    "        pred = m2(xb)\n",
    "        loss = criterion(pred, yb)\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = m1(x_train)\n",
    "        acc = pred.data.max(1)[1].eq(y_train.data).sum()/len(x_train) * 100\n",
    "        loss = criterion(pred, y_train)\n",
    "    print(f\"{time.time() - start} sec - loss : {loss} / acc : {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c8845-4be1-4ec6-9b6e-ca0f2a8c7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m1(x_test.to(device=device))\n",
    "acc = pred.data.max(1)[1].eq(y_test.to(device=device).data).sum()/len(x_test) * 100\n",
    "loss = criterion(pred, y_test.to(device=device))\n",
    "print(f\"loss : {loss} / acc : {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac455e12-49da-425e-91ed-e7bd1d8458ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
